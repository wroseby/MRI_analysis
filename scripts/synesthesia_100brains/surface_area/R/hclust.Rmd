---
title: "Hierarchical clustering of HCP parcels"
output:
  pdf_document: default

params:
  seed: 91939
---
## Absolute surface area
**Prerequisites:** surface area data for 360 HCP parcels in .csv format with metadata in columns 1 through 8.  

This is a notebook to perform hierarchical clustering of partial correlations between HCP parcels. The purpose of this approach is to reduce the number of nodes to below 100 so we can use partial correlations to form networks for a synesthete sample of n = 102 participants. In general, clustering will be performed on partial correlations of left-right averaged data from the control sample of n = 650 participants. However, we will run diagnostics on the clusters, including examining consistency between multiple resamples of n = 300 controls. Partial correlations will be clustered using Euclidean distance as a measure of distance between parcels:

$$ d(p,q) =  \sqrt{\sum_{i=1}^{n}{(q_i-p_i)^2}}$$
Once an optimal clustering is found via diagnostic measures, we will summarise the results within clusters before exporting the results for network analysis with python.

```{r load_packages, include=F}
# Load packages #

library(ppcor)
library(dendextend)
library(dplyr)
library(viridis)
library(ggplot2)
library(reshape2)
library(mclust)
library(cluster)
library(clValid)
library(MatchIt)
library(gridExtra)
library(copula)
library(effsize)
library(MASS)
library(MVN)
```

```{r functions, include=F}

# Define functions #

# Function to normalise a range of numbers to 0-1
normalise = function(x) (x - min(x, na.rm = T)) / (max(x, na.rm = T) - min(x, na.rm = T))

# Function to average data across left and right parcels
lr_average = function(data){ #
  parcel_data = data[,grepl(pattern='^[L,R]_', x=colnames(data))] 
  parcels_LR = unique(sub("^[LR]_", "", colnames(parcel_data))) 
  data_lr = list() 
  for (parcel in parcels_LR) {
    l_col = paste0("L_", parcel)
    r_col = paste0("R_", parcel)
    data_lr[[parcel]] = rowMeans(parcel_data[, c(l_col, r_col)], na.rm = TRUE) 
  }
  data_lr = as.data.frame(data_lr) 
  data_lr = cbind(data[,!grepl(pattern='^[L,R]_', x=colnames(data))], data_lr) 
  colnames(data_lr) = gsub(x = colnames(data_lr), pattern = '^X', replacement = '') 
  return(data_lr)
}

# Function to test for multivariate normality
MVN_test = function(data){
  # mahalanobis distance plot
  mu = colMeans(data)
  sigma = cov(data)
  mdist = mahalanobis(data, mu, sigma)
  qqplot(qchisq(ppoints(nrow(data)), df = ncol(data)), mdist, 
       main="Q-Q Plot of Mahalanobis Distances",
       xlab = 'chi-square distribution',
       ylab = 'Mahalanobis distance')
  abline(0, 1, col="red")

  # mvn tests
  print(mvn(data, mvnTest = "mardia")[["multivariateNormality"]])
  print(mvn(data, mvnTest = "hz")[["multivariateNormality"]])
  print(mvn(data, mvnTest = "royston")[["multivariateNormality"]])
}

# Function to perform hierarchical cluster and plot dendrogram
hclust.plot = function(data, group, clust_method, cut = 2, clustn, plot_tree = T){
 
  # generate correlation matrix - partial pearson
  cormat = pcor(x = data[data$Group == group, 8:ncol(data)],
           method = 'pearson')$estimate
  
  # euclidean distance, can account for positive and negative correlations    
  distance = dist(cormat, method = 'euclidean')
  # 'single' to do single linkage, 'ward.D' to do Ward's method
  clust = hclust(distance, method = clust_method) 
  
  # plot dendrogram
  if (plot_tree == T){
  dend = as.dendrogram(clust) # convert to dendrogram
  dend %>% 
    set('branches_k_color', viridis(clustn), k = clustn) %>% # colour branches by cluster
    set('branches_lwd', 1.8) %>% # set linewidth
    set('labels_cex', 0.15) %>% # set label size
    plot(main = paste0(clustn,' clusters of 180 parcels')) # plot with title
  }
  
  # reorder original cormat to match clustering order and re-generate clustering
  cormat = cormat[clust$order, clust$order] # reorder
  distance = dist(cormat, method = 'euclidean') 
  clust = hclust(distance, method = clust_method)
  
  # acquire cluster identities
  # specify cluster number for single linkage
  if (clust_method == 'single'){
    clusters = stats::cutree(clust, k = clustn)
  }
  # wrdify cut level for Ward's method
  if (clust_method == 'ward.D'){
    clusters = stats::cutree(clust, h = cut) 
  }
  
  # create dataframe
  clusters = data.frame(Area = names(clusters), cluster = clusters)
  
  # return the cluster assignments and the distance matrix for diagnostics
  return(list(clusters, distance))
} 

# Function to find the statistical mode
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

# Function to summarise data within clusters for both controls and synesthetes
summarise_clusters = function(data, clusters){ 
  # convert parcel data to long form for merging
  data_long = melt(data[,c(1,8:ncol(data))]) 
  # rename columns to ensure correct merge
  colnames(data_long) = c('ID', 'Area', 'value') 
  # merge data with the cluster assignments
  data_long = merge(data_long, clusters, by = 'Area')
  
  # summarise within clusters - sum and mean of parcel statistic, plus mean of 2D coordinates and broad region
  data_summarised = data_long %>% 
    group_by(ID, cluster) %>%
    summarise(sum_value = sum(value),
              mean_value = mean(value), 
              x = mean(x), 
              y = mean(y),
              Region = first(Region)) 
  
  # convert back to wide form and bind with metadata
  data_wide_sum = dcast(data_summarised, ID ~ cluster, value.var = 'sum_value')
  data_wide_sum = merge(data[,1:7], data_wide_sum, by = 'ID')
  data_wide_mean = dcast(data_summarised, ID ~ cluster, value.var = 'mean_value')
  data_wide_mean = merge(data[,1:7], data_wide_mean, by = 'ID')
  
  # separately get the centroids of x and y positions for cluster positions
  data_pos = unique(dplyr::select(ungroup(data_summarised), 2,5,6,7)) # select cluster column, mean x and mean y columns
  
  # return the summed values, mean values and cluster positions for plotting
  return(list(data_wide_sum, data_wide_mean, data_pos))
}

# Function to iterate through a desired clustering cutoff and report diagnostics
optimal_cluster = function(data, clust_method, cut_values, seed){
  # get lr-averaged data from full data
  data_lr = lr_average(data)
  
  # initialise diagnostic scores
  mean_silhouettes = vector()
  dunn_indices = vector()
  lr_aris = vector()
  resample_mean_aris = vector()
  cluster_number = vector()

  # iterate through cutoff values
  for (i in seq_along(cut_values)){
    # set cluster number if single linkage approach, otherwise leave at 1
    if (clust_method == 'single'){
    clustn = cut_values[i]
    }
    else {
      clustn = 1
    }
    
    # perform clustering
    clustering = hclust.plot(data_lr, group = 'Control', clust_method = clust_method, 
                             cut = cut_values[i], clustn = clustn, plot_tree = F)
  
    # internal metrics - silhouette scores and Dunn indices
    sil_scores = silhouette(x = as.integer(clustering[[1]]$cluster), dist = clustering[[2]])
    mean_silhouettes[i] = mean(sil_scores[,"sil_width"]) 
    dunn_indices[i] = dunn(distance =clustering[[2]], clusters = as.integer(clustering[[1]]$cluster))
  
    # external consistency - adjusted Rand index for clustering of left vs right
    set.seed(seed)
    data_l = cbind(data[,1:7], data[,grep(x = colnames(data), pattern = '^L_')])
    clustering_l = hclust.plot(data_l, group = 'Control',  clust_method = clust_method, 
                                     clustn = clustn, cut = cut_values[i], plot_tree = F)
    data_r = cbind(data[,1:7], data[,grep(x = colnames(data), pattern = '^R_')])
    clustering_r = hclust.plot(data_r, group = 'Control', clust_method = clust_method, 
                                     clustn = clustn, cut = cut_values[i], plot_tree = F)  
    lr_aris[i] = adjustedRandIndex(clustering_l[[1]]$cluster, clustering_r[[1]]$cluster) 
  
    # external consistency - adjusted Rand index for 20 resamples of 325 participants
    # 20 samples of 325 w/o replacement guarantees < .1% chance of missing any observation
    clustering_resamp = list()
    set.seed(seed)
    for (j in seq(1:20)){ 
      # take resamples and cluster
      data_lr_resamp = data_lr[data_lr$Group == 'Control',][sample(1:650, size = 350, replace = F),] 
      clustering_resamp[[j]] = hclust.plot(data_lr_resamp, group = 'Control', clust_method = clust_method, 
                                           clustn = clustn, cut = cut_values[i], plot_tree = F)
    }
    # get adjusted Rand index for every combination of the 10 clusterings
    clustering_resamp_ari = vector() 
    for (j in 1:ncol(combn(20,2))){ 
      resamp1 = clustering_resamp[[combn(20,2)[,j][1]]][[1]]
      resamp2 = clustering_resamp[[combn(20,2)[,j][2]]][[1]]
      clustering_resamp_ari[j] = adjustedRandIndex(resamp1[order(resamp1$Area),]$cluster,
                                                resamp2[order(resamp2$Area),]$cluster)
    }
    # then get the mean ARI
    resample_mean_aris[i] = mean(clustering_resamp_ari)
    
    # get the cluster number
    cluster_number[i] = length(unique(clustering[[1]]$cluster))

  }
  # normalise the scores
  mean_silhouettes_n = normalise(mean_silhouettes)
  dunn_indices_n = normalise(dunn_indices)
  lr_aris_n = normalise(lr_aris)
  resample_mean_aris_n = normalise(resample_mean_aris)
  
  # aggregate scores and find the optimal clustering
  agg_score = (mean_silhouettes_n + dunn_indices_n + lr_aris_n + resample_mean_aris_n)
  optimal_cut = cut_values[which(agg_score == max(agg_score, na.rm = T))]
  optimal_cluster_number = cluster_number[which(agg_score == max(agg_score, na.rm = T))]
  
  # return the recommended cut and the table of scores
  return(list(optimal_cut, agg_score, optimal_cluster_number))
}

# Function to calculate and plot partial correlation distributions according to a grouping variable
plot_pcors = function(data_list, nclust, group_vec){
  # generate partial correlations
  pcor_list = lapply(data_list, FUN = function(df){pcor(df[,8:ncol(df)])$estimate})
  # melt to long form
  pcor_list = lapply(pcor_list, FUN = function(df){df = melt(df)})
  # add desired grouping variable
  for (i in 1:length(pcor_list)){
    pcor_list[[i]]$Group = group_vec[i]
  }
  # bind into one dataframe
  pcors = bind_rows(pcor_list, .id = 'Sample')
  
  # plot partial correlation distributions
  pcors$Group = factor(pcors$Group, levels = unique(group_vec))
  pcors$Sample = factor(pcors$Sample)
  ggplot(data = pcors, aes(x = value, color = Group, group = Sample))+ 
    geom_density(lwd = 1.05)+
    scale_color_manual(values = c('blue', 'red'))+
    labs(x = 'Partial correlation', y = 'Density')+
    theme(legend.position = 'top')
}

# Function to generate copula and produce sample data
sampleCopula = function(data, log = T, 
                        copulatype = c('gaussian', 'empirical', 't'), invert = c('gaussian', 'empirical'),
                        n = 102, n_samp = 10){
  
  # initialise results
  copula_res = list()
  
  # filter out metadata
  metadata = data[,1:7]
  data = data[,8:ncol(data)]
  
  # log-transform data if desired
  if (log == T) {data = log(data)}
  
  # get pseudo-observations (empirical data transformed to uniform distribution)
  pseudoobs = pobs(data) 
  
  # generate copula
  # unstructured gaussian copula
  if (copulatype == 'gaussian'){
    #empirical_cor = cor(qnorm(pseudoobs), method = "pearson") # for ml methods (too slow)
    #start_values = empirical_cor[lower.tri(empirical_cor)]
    copula_norm = normalCopula(dim = ncol(data), dispstr = 'un')
    copula_fit = fitCopula(copula_norm, pseudoobs, method = "irho")
    copula = copula_fit@copula
  }
  # empirical copula
  if (copulatype == 'empirical'){
    copula = empCopula(pseudoobs, smoothing = 'none')
  }
  # t-copula
  if (copulatype == 't'){
    copula_t = tCopula(dim = ncol(data))
    copula_fit = fitCopula(copula_t, pseudoobs)
    copula = copula_fit@copula
  }
  
  # get samples from copula
  copula_samples = list()
  for (i in 1:n_samp){
    copula_samples[[i]] = rCopula(n, copula)
  
    # invert copula samples
    # inversion with gaussian cdf
    if (invert == 'gaussian'){
      samples = matrix(0, nrow = nrow(copula_samples[[i]]), ncol = ncol(copula_samples[[i]]))
      for (j in 1:ncol(copula_samples[[i]])) {
        samples[,j] = qnorm(copula_samples[[i]][,j], mean = mean(data[,j]), sd = sd(data[,j]))
      }
    }
    # inversion with empirical cdf
    if (invert == 'empirical'){
      samples = list()
      for (j in 1:ncol(copula_samples[[i]])){
        ecdf_col = ecdf(data[,j]) 
        inv_ecdf = approxfun(ecdf_col(as.matrix(data)[,j]), as.matrix(data)[,j], method = "linear", rule = 2) 
        samples[[paste0("Cluster", j)]] = inv_ecdf(copula_samples[[i]][,j])
      }
    }
    
    # compile samples
    samples = do.call(cbind, samples)
    results = list()
    results[[1]] = samples
    if (log == T){results[[1]] = exp(results[[1]])}
    results[[1]] = cbind(metadata, results[[1]])
    results[[2]] = mean(abs(cor(samples, method = 'pearson') - cor(data, method = 'pearson')))
    names(results) = c('copula_samples', 'cor_error')

    copula_res[[i]] = results
    
  }
  
  return(copula_res)
  
}

```

```{r load_data, include=F}
# Read in data and set paths for saving #
datapath =  'D:/Documents/Academia/projects/ward_lab/MRI_analysis/datasets/synesthesia_100brains/'
savepath_data = 'D:/Documents/Academia/projects/ward_lab/MRI_analysis/shared/synesthesia_100brains/surface_area/'
savepath_outputs = 'D:/Documents/Academia/projects/ward_lab/MRI_analysis/outputs/synesthesia_100brains/surface_area/R/'
SA_abs = read.csv(paste0(datapath, '/surface_area/biomarker_S1C_harm.csv'), row.names = 1)
SA_rel = read.csv(paste0(datapath, '/surface_area/biomarker_S1A_harm.csv'), row.names = 1)
parcel_positions = read.csv(paste0(datapath, '/common/parcel_positions_flat.csv'))
SA_abs_lr = lr_average(SA_abs)
```

# Ward's clustering

For hierarchical clustering, we will use Ward's method, since it is known to form spherical clusters and well accommodate outliers. This method forms clusters by minimising the within cluster variance.

To obtain a specific clustering here, we have to choose a value which specifies where the clustering will be cut. Nodes separated below this value will be in the same cluster, while nodes separated above this value will be in different clusters. We will iterate across some cutoff values which are known (via prior testing) to generate less than 100 clusters but more than 1 cluster. We iterate in steps of 0.01 which will produce a small change in the cluster number in most cases.

Additionally, because the process of finding the optimal cluster depends to some degree on random subsampling of control data, we will iterate the process over 10 random seeds to see if there is a stable optimal cluster number. The code below will perform this iteration to find the optimal cutoff value and the number of clusters associated with it.

```{r ward_clustering_optimisation, message=F, warning=F}
# Set random seeds
seeds = c(674345, 811071, 276503, 411732, 475378, 
          690070, 425943, 729736, 621889, 660812)
# Set cutoff values to test
cut_values = seq(from = 1.40, to = 2.50, by = 0.01)
# Initialise results
clust_wrd_clustns = vector()
clust_wrd_cuts = vector()
# Iterate over the 10 seeds
for (i in 1:10){
  clust_wrd_optimum = optimal_cluster(data = SA_abs, 
                clust_method = 'ward.D', 
                cut_values = cut_values, 
                seed = seeds[i])
  clust_wrd_clustns[i] = clust_wrd_optimum[[3]]
  clust_wrd_cuts[i] = clust_wrd_optimum[[1]]
}
clustn = Mode(clust_wrd_clustns)
cut = Mode(clust_wrd_cuts)
print(paste0('Optimal cluster numbers: ', clust_wrd_clustns))
print(paste0('Consensus cluster number = ', clustn))
```

It seems here the optimal number of clusters is `r print(clustn)`, which is an appropriate number for a network analysis. Let's reform the clustering, and plot the dendrogram and the cluster locations on the 2D flattened cortical sheet:

```{r cluster_locations}
# Form optimal clusters and plot dendrogram
clust_wrd = hclust.plot(SA_abs_lr, group = 'Control', clust_method = 'ward.D', 
                        cut = cut, clustn = clustn, plot_tree = T)

# Plot cluster anatomical positions
clust_wrd_pos = merge(parcel_positions, clust_wrd[[1]]) # merge with positions
clust_wrd_pos$cluster = factor(clust_wrd_pos$cluster) # factor clusters
ggplot(data = clust_wrd_pos, aes(x = x, y = max(y) - y, fill = cluster))+
  geom_point(size = 0.8, shape = 21, stroke = 0.5)+
  geom_text(aes(
    label = gsub(x = cluster, pattern = 'cluster ', replacement = '')), 
    size = 2, vjust = -0.6)+
  scale_fill_viridis_d(option = "C")+
  labs(x = 'x', y = 'y')+
  theme(legend.position = 'none',
        axis.text = element_blank())

```
The clusters seem to form according to anatomical location, with proximal parcels falling into the same cluster. The number of parcels in a cluster varies from about 1 to 6. This indicates the clustering is uncovering some anatomical structure in the data.

Let's summarise the data within the clusters and export it to a .csv file for analysis in python. Since we want to be able to carry out some statistical comparison with empirical data, we will perform the summarising for 6 unique subsamples of n = 102 from the control dataset of n = 650, to match the synesthete sample size. While we're here, we'll also plot the partial correlation distributions for the control subsamples and synesthetes:

```{r cluster_unique_subsamples, warning=F, message=F}
# Generate 6 groups of 102 random sample numbers
set.seed(params$seed)
sample_numbers = sample(1:650, size = 6 * 102, replace = F)
sample_numbers = split(sample_numbers, ceiling(seq_along(sample_numbers)/102))

# Sample controls, summarise within clusters and write to .csv
SA_abs_wrd_sum_102 = list()
for (i in seq(1:6)){
  SA_abs_lr_cntrl_samp = 
    SA_abs_lr[SA_abs_lr$Group == 'Control',][sample_numbers[[i]],]
  SA_abs_wrd_sum_102[[i]] = 
    summarise_clusters(SA_abs_lr_cntrl_samp, clust_wrd_pos)
  write.csv(SA_abs_wrd_sum_102[[i]][[1]], 
            file = paste0(savepath_data, 
                          'hclust/SA_abs_wrd_sum_cntrl_samp_', i, '.csv'), 
            row.names = F)
}
# Summarise clusters for synesthetes and write to .csv
SA_abs_wrd_sum_syn = summarise_clusters(SA_abs_lr[SA_abs_lr$Group == 'Syn',], 
                                        clust_wrd_pos)
write.csv(SA_abs_wrd_sum_syn[[1]], 
          file = paste0(savepath_data, 'hclust/SA_abs_wrd_sum_syn.csv'), 
          row.names = F) 

# Write one .csv for cluster locations (parcel centroids)
write.csv(SA_abs_wrd_sum_syn[[3]], 
          file = paste0(savepath_data, 'hclust/SA_abs_wrd_pos.csv'), 
          row.names = F) 

# Compile subsamples into one list
SA_abs_wrd_samp = lapply(SA_abs_wrd_sum_102, FUN = function(list){list[[1]]})
SA_abs_wrd_samp[[7]] = SA_abs_wrd_sum_syn[[1]]

# Plot partial correlation distributions
plot_pcors(data_list = SA_abs_wrd_samp, nclust = nclust, 
           group_vec = c(rep('Control', 6), 'Syn'))
```


This has exported 6 control files and one synesthete file to the shared data folder, under 'hclust'. We'll use this data with python for further analysis. The correlation distributions would suggest that perhaps the synesthetes appear a bit more variably spread than the controls here.

Let's import the results from the network analysis in python and visualise them here. 

```{r unique_subsamples_net_metrics}
# Import results
net_metrics = read.csv(paste0(savepath_data, 
                              'hclust/clust_wrd_samp_net_metrics.csv'))
net_metrics = melt(net_metrics, id.vars = 'Group')
net_metrics$Group = factor(net_metrics$Group, levels = c('Control', 'Syn'))

# Plot results
net_metrics$x = 1
net_metrics[net_metrics$Group == 'Syn',]$x = 2
net_metrics = net_metrics %>% mutate(
    x_jittered = ifelse(Group == "Control", 1 + runif(n(), -0.1, 0.1), x))

ggplot(data = net_metrics, aes(x = x, y = value, fill = Group))+
  stat_summary(fun = mean, geom = 'col', alpha = 0.5, width = 0.8)+
  geom_point(aes(x = x_jittered, colour = Group), 
             shape = 21, stroke = 1, size = 2, fill = NA, show.legend = F)+
  scale_fill_manual(values = c('blue', 'red'))+
  scale_colour_manual(values = c('blue', 'red'))+
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    legend.position = 'top')+
  guides(fill = guide_legend(override.aes = list(alpha = 1)))+
  facet_wrap(~variable, scales = 'free_y', 
             labeller = as_labeller(c(clustering = 'Clustering',
                                      efficiency = 'Efficiency',
                                      L_obs = 'Path length',
                                      mean_eb = 'Mean edge betweenness',
                                      mean_vb = 'Mean vertex betweenness')))
```

It does appear like the synesthetes have greater clustering and efficiency, and lower characteristic path length the the control subsamples.

However, in testing, the results seem to change depending on the seed we use to generate the control subsamples. This might have something to do with other variables like age, sex and scan site. Below, we will generate some matched control samples based on these three variables and compare the effects of these of the partial correlation distributions.


# Effects of covariates on cluster partial correlations

## Sex

We start by looking at sex. Let's first check how many of each sex we have in our control sample:

```{r sex_n}
print(paste0('Number of females = ', nrow(SA_abs[SA_abs$Sex == 1,])))
print(paste0('Number of males = ', nrow(SA_abs[SA_abs$Sex == 2,])))
```

Since we only have 178 males, we will have to select an equal (or smaller) number of females for comparison. These should also be as close in possible in age and dataset origin. To achieve this we'll employ optimal matching using the MatchIt package.

Once we have our matched samples, let's summarise data within our set of clusters and plot the partial correlations for each. Note that the `plot_pcors()` function requires data split into a list, along with a grouping vector that describes the group of each set of data in the list.

```{r match_sexes, message=F, warning=F}
# Convert categorical variables to factors
SA_abs$Sex = factor(SA_abs$Sex)
SA_abs$Scan = factor(SA_abs$Scan)

# Match the control data by sex
SA_abs_cntrl = SA_abs[SA_abs$Group == 'Control',]
matched = matchit(Sex ~ Age + Scan, data = SA_abs_cntrl, 
                  method = "optimal", ratio = 1)
SA_abs_matched = match.data(matched)
print(table(SA_abs_matched$Sex))

# Remove additional columns, lr average, and summarise within clusters
SA_abs_matched = SA_abs_matched[,-c(368:370)]
SA_abs_matched = lr_average(SA_abs_matched)
SA_abs_matched_clust = summarise_clusters(SA_abs_matched, clust_wrd_pos)
SA_abs_matched_clust = split(SA_abs_matched_clust[[1]], 
                             SA_abs_matched_clust[[1]]$Sex)

# Plot partial correlation distributions
plot_pcors(data_list = SA_abs_matched_clust, nclust = nclust, 
           group_vec = c('Female', 'Male'))

```

It looks like males and females are broadly similar here, with males having a bit of bias towards more negative correlations.

## Age

Since age is a numerical variable and not suitable for plotting different groups, we will group ages into three categories: <22, 22-35, and >35 to see if there are any differences. However, because matching requires splitting into two groups, we will have to perform pairwise comparisons.

```{r match_age_groups, message=F, warning=F}
# Create age groups
SA_abs_cntrl = SA_abs[SA_abs$Group == 'Control',]
SA_abs_cntrl$Age_group = ifelse(SA_abs_cntrl$Age < 22, 
                                'Young', 'Middle')
SA_abs_cntrl$Age_group = ifelse(SA_abs_cntrl$Age > 35, 
                                'Old', SA_abs_cntrl$Age_group)
SA_abs_cntrl$Age_group = factor(SA_abs_cntrl$Age_group, 
                                levels = c('Young', 'Middle', 'Old'))
SA_abs_cntrl$Age = SA_abs_cntrl$Age_group # replace age column
SA_abs_cntrl = SA_abs_cntrl[,-368] # remove age groups column

# Iterate over pairwise comparisons
age_combos = combn(c('Young', 'Middle', 'Old'), 2)
plots = list()
for (i in 1:ncol(age_combos)){
  matched = matchit(Age ~ Sex + Scan, 
                    data = 
                      SA_abs_cntrl[SA_abs_cntrl$Age == age_combos[,i][1] 
                                   | SA_abs_cntrl$Age == age_combos[,i][2],], 
                    method = "optimal", ratio = 1)
  SA_abs_matched = match.data(matched)
  SA_abs_matched = SA_abs_matched[,-c(368:370)]
  SA_abs_matched = lr_average(SA_abs_matched)
  print(table(SA_abs_matched$Age))
  SA_abs_matched_clust = summarise_clusters(SA_abs_matched, clust_wrd_pos)
  SA_abs_matched_clust[[1]]$Age = factor(SA_abs_matched_clust[[1]]$Age, 
                                         levels = unique(age_combos[,i]))
  SA_abs_matched_clust = split(SA_abs_matched_clust[[1]], 
                               SA_abs_matched_clust[[1]]$Age)
  plots[[i]] = plot_pcors(data_list = SA_abs_matched_clust, nclust = nclust, 
                          group_vec = age_combos[,i])
}

grid.arrange(grobs = plots, ncol = 3, widths = c(3,3,3))
```

Some small differences between some of the age groups here. The differences in the spread of the distributions between comparisons is likely explained by the differences in sample sizes in each matched sample. 

## Scan type

Let's start by checking the numbers of different scan types in the data.

```{r check_scan_types}
# Check numbers of scan types
print(table(SA_abs$Scan))
```

Because the 'HCP_YA_CISC' numbers are low, we will have to discard this scan type and do three pairwise comparisons. Let's implement this as we did above for the ages groups:

```{r match_scan_types, message=F, warning=F}
# Remove YA_CISC scans
SA_abs_cntrl = SA_abs[SA_abs$Group == 'Control',]
SA_abs_cntrl = SA_abs[SA_abs$Scan != 'HCP_YA_CISC',]
SA_abs_cntrl$Scan = factor(SA_abs_cntrl$Scan, 
                           levels = unique(SA_abs_cntrl$Scan))

# Iterate over pairwise comparisons
scan_combos = combn(c('HCP_YA_Database', 'HCP_DA_Database', 'HCP_DA_CISC'), 2)
plots = list()
for (i in 1:ncol(age_combos)){
  matched = matchit(Scan ~ Sex + Age, 
                    data = 
                      SA_abs_cntrl[SA_abs_cntrl$Scan == scan_combos[,i][1] 
                                   | SA_abs_cntrl$Scan == scan_combos[,i][2],], 
                    method = "optimal", ratio = 1)
  SA_abs_matched = match.data(matched)
  SA_abs_matched = SA_abs_matched[,-c(368:370)]
  SA_abs_matched = lr_average(SA_abs_matched)
  print(table(SA_abs_matched$Scan))
  SA_abs_matched_clust = summarise_clusters(SA_abs_matched, clust_wrd_pos)
  SA_abs_matched_clust[[1]]$Scan = factor(SA_abs_matched_clust[[1]]$Scan, 
                                         levels = unique(scan_combos[,i]))
  SA_abs_matched_clust = split(SA_abs_matched_clust[[1]], 
                               SA_abs_matched_clust[[1]]$Scan)
  plots[[i]] = plot_pcors(data_list = SA_abs_matched_clust, nclust = nclust, 
                          group_vec = scan_combos[,i])
}

grid.arrange(grobs = plots, ncol = 3)
```

Some differences here between the CISC data and the HCP datasets in partciular.

## Matched controls and synesthetes

Since all these covariates appear to affect the correlation distributions to at least some degree, let's now compare the 102 synesthetes with 102 controls matched according to these covariates. 

```{r match_groups, message=F, warning=F}
# Match controls and synesthetes by sex and age
SA_abs$Group = factor(SA_abs$Group, levels = c('Control', 'Syn'))
matched = matchit(Group ~ Sex + Age + Scan, data = SA_abs, 
                  method = "optimal", ratio = 1)
SA_abs_matched = match.data(matched)

# Remove additional columns, lr average, and summarise within clusters
SA_abs_matched = SA_abs_matched[,-c(368:370)]
SA_abs_matched = lr_average(SA_abs_matched)
SA_abs_matched_clust = summarise_clusters(SA_abs_matched, clust_wrd_pos)
SA_abs_matched_clust = split(SA_abs_matched_clust[[1]], 
                             SA_abs_matched_clust[[1]]$Group)

# Plot partial correlation distributions
plot_pcors(data_list = SA_abs_matched_clust, nclust = nclust, 
           group_vec = c('Control', 'Syn'))

# Write to python
write.csv(SA_abs_matched_clust$Control, 
          file = paste0(savepath_data, 
                        'hclust/SA_abs_wrd_sum_cntrl_matched.csv'),
          row.names = F)
```

Here we see a more obvious difference in the correlation distributions. However, since there is only a small number of CISC scans, matching by scan may be leading to some mismatches of age or sex. Let's check:

```{r, check_covariates}
print(mean(SA_abs_matched[SA_abs_matched$Group == 'Control',]$Age))
print(mean(SA_abs_matched[SA_abs_matched$Group == 'Syn',]$Age))

print(table(SA_abs_matched[SA_abs_matched$Group == 'Control',]$Sex))
print(table(SA_abs_matched[SA_abs_matched$Group == 'Syn',]$Sex))
```
Indeed, there are large differences in both age and sex which are not really acceptable for a matched sample. We will have to ignore matching by scan for now to ensure matched ages and sexes. 

```{r match_groups_noscan, message=F, warning=F}
# Match controls and synesthetes by sex and age
SA_abs$Group = factor(SA_abs$Group, levels = c('Control', 'Syn'))
matched = matchit(Group ~ Sex + Age, data = SA_abs, 
                  method = "optimal", ratio = 1)
SA_abs_matched = match.data(matched)

# Remove additional columns, lr average, and summarise within clusters
SA_abs_matched = SA_abs_matched[,-c(368:370)]
SA_abs_matched = lr_average(SA_abs_matched)
SA_abs_matched_clust = summarise_clusters(SA_abs_matched, clust_wrd_pos)
SA_abs_matched_clust = split(SA_abs_matched_clust[[1]], 
                             SA_abs_matched_clust[[1]]$Group)

# Plot partial correlation distributions
plot_pcors(data_list = SA_abs_matched_clust, nclust = nclust, 
           group_vec = c('Control', 'Syn'))

# Write matched controls to csv
write.csv(SA_abs_matched_clust$Control, 
          file = paste0(savepath_data, 
                        'hclust/SA_abs_wrd_sum_cntrl_matched.csv'), 
          row.names = F)

# Check covariates
print(mean(SA_abs_matched[SA_abs_matched$Group == 'Control',]$Age))
print(mean(SA_abs_matched[SA_abs_matched$Group == 'Syn',]$Age))

print(table(SA_abs_matched[SA_abs_matched$Group == 'Control',]$Sex))
print(table(SA_abs_matched[SA_abs_matched$Group == 'Syn',]$Sex))
```

There is now a much better match of ages between the two groups. Although the sexes are unbalanced within each group, the relative proportions are also very similar between the groups.

The differences in the distribution of partial correlations also appear similar to before, with synesthetes having more variable correlations than controls. Let's write this data to .csv for analysis in python.

```{r export_matches}
# Write matched controls to csv
write.csv(SA_abs_matched_clust$Control, 
          file = paste0(savepath_data, 
                        'hclust/SA_abs_wrd_sum_cntrl_matched.csv'), 
          row.names = F)
```

Analysis in python shows that there are indeed the expected differences between the macthed synesthete and control samples in terms of basic network metrics. Let's import the image files to visualise the networks:

```{r matched_control_network, fig.cap = 'Partial correlation network of 62 SA clusters in controls'}
# Import network visualisations
python_imports = 'D:/Documents/Academia/projects/ward_lab/MRI_analysis/outputs/synesthesia_100brains/surface_area/python/'
knitr::include_graphics(paste0(python_imports, 'SA_abs_matched_clust_cntrl_net.png'))
```

```{r matched_syn_network, fig.cap = 'Partial correlation network of 62 SA clusters in synesthetes'}
knitr::include_graphics(paste0(python_imports, 'SA_abs_matched_clust_syn_net.png'))
```

In our python analysis we've also matched the edge weights to the 2D distance between clusters. Let's import the data to visualise this relationship:

```{r weight_by_dist, warning=F, message=F}
# Import and combine 2D distance data
dists_cntrl = read.csv(paste0(savepath_data,
                              'hclust/clust_wrd_matched_cntrl_dists.csv'))
dists_syn = read.csv(paste0(savepath_data,
                            'hclust/clust_wrd_matched_syn_dists.csv'))
dists = rbind(dists_cntrl, dists_syn)
dists$Group = rep(c('Control', 'Syn'), each = nrow(dists)/2)

ggplot(data = dists, aes(x = distance, y = weight, colour = Group))+
  geom_smooth(lwd = 1.1)+
  scale_color_manual(values = c('blue', 'red'))+
  labs(x = 'Two-dimensional distance (pixels)', # change to mm at some point
       y = 'Absolute partial correlation')
```

While controls appear to have stronger relationships at shorter distances, in synesthetes the relationship is flatter with stronger correlations at longer distances compared to controls.

## Cluster number perturbation

We want to make sure that at least the basic network differences are robust to small changes in the number of clusters. Since spatial resolution / node number is known to affect network properties, we want to choose some cluster numbers close to the optimal for testing.

In the code below, we will iterate over four values close to the optimal cutoff (3 above, 3 below) and examine the summarised data for the matched control sample. 

```{r alternative_clusters, message=F, warning=F}
# Set cutoff values
cuts = c(cut - 0.03, cut - 0.025, cut - 0.02, 
         cut + 0.01, cut+ 0.02, cut + 0.035)

# Initialise results
plots = list()
SA_abs_matched_clust_alts = list()

# Iterate over cutoffs
for (i in 1:length(cuts)){
  # Perform clustering
  clust_wrd_alt = hclust.plot(SA_abs_lr, group = 'Control', 
                              clust_method = 'ward.D', cut = cuts[i], 
                              clustn = clustn, plot_tree = F)
  
  # Print number of clusters
  nclust_alt = max(clust_wrd_alt[[1]]$cluster)
  print(paste0('Number of clusters = ', nclust_alt))
  
  # Merge with position data
  clust_wrd_alt_pos = merge(parcel_positions, clust_wrd_alt[[1]])
  clust_wrd_alt_pos$cluster = factor(clust_wrd_alt_pos$cluster) 
  
  # Summarise the matched control data by the clusters
  SA_abs_matched_clust_alt = summarise_clusters(SA_abs_matched, 
                                                clust_wrd_alt_pos)
  SA_abs_matched_clust_alt = split(SA_abs_matched_clust_alt[[1]], 
                                   SA_abs_matched_clust_alt[[1]]$Group)
  
  # Get control data and plots
  SA_abs_matched_clust_alts[[i]] = SA_abs_matched_clust_alt
  plots[[i]] = plot_pcors(data_list = SA_abs_matched_clust_alt, 
                          nclust = nclust_alt, group_vec = c('Control', 'Syn'))
}

grid.arrange(grobs = plots, ncol = 3)

# Export to .csv
for (i in 1:length(SA_abs_matched_clust_alts)){
  write.csv(SA_abs_matched_clust_alts[[i]]$Control, 
          file = paste0(savepath_data, 
                        'hclust/SA_abs_wrd_sum_cntrl_matched_alt_', i, '.csv'), 
          row.names = F)
  write.csv(SA_abs_matched_clust_alts[[i]]$Syn, 
          file = paste0(savepath_data, 
                        'hclust/SA_abs_wrd_sum_syn_matched_alt_', i, '.csv'), 
          row.names = F)
}
```

This process has tested cluster numbers 59, 60, 61, 63 64, and 65. In each case, we see that the synesthete partial correlations are distributed more widely than the matched controls, as was observed in the optimal 62 cluster case. This likely demonstrates that the main structure of our results is not drastically affected by small changes to the number of clusters away from 62. 

To confirm this result, we performed network analysis in python using the exported .csv files. The network analysis indicates that the results are robust across these changes in cluster number. Let's import the metrics and visualise clustering and efficiency for each cluster number.

```{r alternative_clusters_net_metrics, message=F, warning=F}
net_metrics = read.csv(paste0(savepath_data, 
                              'hclust/clust_wrd_matched_alt_net_metrics.csv'))
net_metrics$cluster_number = rep(c(59,60,61,62,63,65), 2)
net_metrics = net_metrics[,c(1,2,6,7)]
net_metrics = melt(net_metrics, id.vars = c('cluster_number','Group'))
net_metrics$Group = factor(net_metrics$Group, levels = c('Control', 'Syn'))
net_metrics$cluster_number = factor(net_metrics$cluster_number)

# Plot results
ggplot(data = net_metrics, aes(x = variable, y = value, fill = Group))+
  geom_col(width = 0.8, position = position_dodge(0.8), 
           alpha = 0.5, colour = 'black')+
  scale_fill_manual(values = c('blue', 'red'))+
  scale_colour_manual(values = c('blue', 'red'))+
  labs(x = 'Metric', y = 'Value')+
  guides(fill = guide_legend(override.aes = list(alpha = 1)))+
  theme(
    legend.position = 'top')+
  facet_wrap(~cluster_number, scales = 'free_y')
```




## Multivariate distribution

Indeed, we can clearly see that both clustering and efficiency are consistently increased in synesthetes across these cluster numbers. This gives some confidence that these results are real group differences, and not an artefact of the clustering process.

Now we want to check if these findings are robust to some changes in the exact samples; i.e., whether these results reflect stable differences in the correlation structure or if they are specific to the empirical samples we used. To test this, and to determine statistical significance, we can use the copula approach to model the correlation structure of both empirical samples, and create new synthetic data based on this structure. 

However, the choice of copula depends on the properties of the population correlation structure. It is typically assumed that this kind of data follows a multivariate normal (or at least elliptical) distribution, for which we can use a gaussian copula. However, we should not just assume this is the case. This question also holds further ramifications for the network analysis of empirical and synthetic data. Ultimately, we are interested in using partial correlations to form a graph that accurately describes the structural relationships between a set of regions of the brain. However, this description is only fully accurate under a gaussian assumption; i.e.: partial correlations will only capture the full dependence structure of the data if it follows a multivariate normal distribution. 

Therefore, we are going to check the MVN assumption holds for 1) our full control data, prior to clustering and 2) our matched data, after clustering. We first examine the Mahalanobis distances of each variable, which should follow a chi-squared distribution. We also perform several tests from the MVN package. Let's first do this for the complete control data used to generate the clusters:

```{r}
# MVN tests for complete control data, prior to clustering
MVN_test(SA_abs_lr[SA_abs_lr$Group == 'Control',8:ncol(SA_abs_lr)])
```

While the tests are not in complete agreement of an MVN distribution, the Mahalanobis distance plot indicates that an MVN assumption is not inappropriate. The results here are likely affected by both the large number of dimensions (n = 180) and the large number of samples for each dimension (n = 650). This means it is easier for the data to fail normality tests.

Let's now check the matched data, after clustering, to ensure our partial correlations appropriately describe the cluster dependence structure in each group:

```{r MVN_tests}
MVN_test(SA_abs_matched_clust$Control[,8:ncol(SA_abs_matched_clust$Control)])
MVN_test(SA_abs_matched_clust$Syn[,8:ncol(SA_abs_matched_clust$Syn)])
```

The results here are similar to the full control data. The distribution of Mahalanobis distances would point towards MVN for both groups. However, the tests are not completely suggestive of this. If we check the distributions of the individual clusters:

```{r}
table(mvn(SA_abs_matched_clust$Control[,8:ncol(SA_abs_matched_clust$Control)])
      [['univariateNormality']]$Normality)
```

We see that most clusters are normally distributed. Therefore, the failure to pass the MVN tests is likely a result of a few clusters not having exactly normal properties, which is not unlikely with this many clusters (62 * 0.05 = 3). 

Therefore, in making the MVN assumption, we can be fairly sure that partial correlations accurately describe conditional dependence structure of our data. Furthermore, this suggests we can use a gaussian copula to accurately model the correlation structure.

## Copula sampling

Given these results, let's generate gaussian copulae based off the cluster-summarised matched log-transformed data, take some samples, and export them to python for analysis:

```{r copula_sampling, message = F, warning = F}
# Generate copula samples
set.seed(params$seed)
SA_abs_matched_clust_cntrl_copula = 
  sampleCopula(data = SA_abs_matched_clust$Control, 
                                                 log = F, 
                                                 copulatype = c('gaussian'), 
                                                 invert = c('empirical'),
                                                 n = 102, 
                                                 n_samp = 10)
set.seed(params$seed)
SA_abs_matched_clust_syn_copula = 
  sampleCopula(data = SA_abs_matched_clust$Syn, 
                                                 log = F, 
                                                 copulatype = c('gaussian'), 
                                                 invert = c('empirical'),
                                                 n = 102, 
                                                 n_samp = 10)
# Export to python
for (i in 1:10){
  write.csv(SA_abs_matched_clust_cntrl_copula[[i]][["copula_samples"]], 
            paste0(savepath_data, 
                   'hclust/SA_abs_matched_clust_cntrl_copula_', i, '.csv'), 
            row.names = F)
  write.csv(SA_abs_matched_clust_syn_copula[[i]][["copula_samples"]], 
            paste0(savepath_data, 
                   'hclust/SA_abs_matched_clust_syn_copula_', i, '.csv'), 
            row.names = F)
}

copula_MAE_cntrl = mean(sapply(SA_abs_matched_clust_cntrl_copula,
                          FUN = function(list){list[["cor_error"]]}))
copula_MAE_syn = mean(sapply(SA_abs_matched_clust_syn_copula,
                          FUN = function(list){list[["cor_error"]]}))                        

print(c(copula_MAE_cntrl, copula_MAE_syn))
```

Note that the mean absolute error for the sampled correlations compared to empirical correlations is quite low for both groups, at < .1. 

Having run the network analysis on these samples in python, let's import the network metrics and compare them across groups.

```{r copula_metrics}
# Import metrics
net_metrics = read.csv(paste0(
  savepath_data, 'hclust/clust_wrd_matched_copula_net_metrics.csv'))
net_metrics = melt(net_metrics, id.vars = 'Group')
net_metrics$Group = factor(net_metrics$Group, levels = c('Control', 'Syn'))

# Run statistics
run_tests = function(data) {
  t_test_result = t.test(value ~ Group, data = data)
  effect_size = cohen.d(value ~ Group, data = data, hedges = T)$estimate
  shapiro_control = shapiro.test(data$value[data$Group == "Control"])
  shapiro_syn = shapiro.test(data$value[data$Group == "Syn"])
  tibble(
    mean_control = mean(data$value[data$Group == "Control"]),
    mean_syn = mean(data$value[data$Group == "Syn"]),
    shapiro_control = shapiro_control$p.value,
    shapiro_syn = shapiro_syn$p.value,
    effect_size = effect_size,
    p_value = t_test_result$p.value
  )
}
net_metrics_summary = net_metrics %>%
  group_by(variable) %>%
  group_modify(~ run_tests(.x)) %>%
  ungroup()

print(net_metrics_summary)

# Plot results
ggplot(data = net_metrics, aes(x = Group, y = value, 
                               fill = Group, colour = Group))+
  stat_summary(fun = mean, geom = 'col', 
               alpha = 0.5, width = 0.8, colour = 'black')+
  geom_point(shape = 21, stroke = 1, size = 2, fill = NA, show.legend = F, 
             position = position_jitter(0.1))+
  scale_fill_manual(values = c('blue', 'red'))+
  scale_colour_manual(values = c('blue', 'red'))+
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.y = element_blank(),
    legend.position = 'top')+
  guides(fill = guide_legend(override.aes = list(alpha = 1)))+
  facet_wrap(~variable, scales = 'free_y', 
             labeller = as_labeller(c(clustering = 'Clustering',
                                      efficiency = 'Efficiency',
                                      L_obs = 'Path length',
                                      mean_eb = 'Mean edge betweenness',
                                      mean_vb = 'Mean vertex betweenness')))
```